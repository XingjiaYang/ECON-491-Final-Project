---
title: "ECON FINAL PROJECT"
author: "Xingjia Yang, Husheng Yu, Naman"
date: "2025-04-19"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

require(doParallel)
cl = makeCluster(detectCores() - 1)
registerDoParallel(cl)
```

# Data Preprocessing

## Read Data

```{r}
require(tidyverse)

raw = read_csv("mortgage.csv")
```

## Clean Data

```{r}
sum(is.na(raw))
```

From the output, I can see that the dataset does NOT missing values.

From the data, some columns' value has quotes, but some do not. Then I observe the data, and I find that the columns with quote is categorical values. Therefore, I will transfer the quote value to factors and eliminate the quotes.

```{r}
df_cleaned = raw %>%
    mutate(across(
        .cols = everything(),
        .fns = ~ {
            
            all_quoted = all(grepl("^'.*'$", .x[!is.na(.x)]))
            
            if (all_quoted) {
                
                as.factor(str_remove_all(.x, "^'|'$"))
                
            } else {
                    
                suppressWarnings(as.numeric(.x))
            }
        }
    )
)
```

<\p>
Another thing, I have find `MORTLINE` only has 3 values. It means it may be a categorical predictors. Therefore, I will transfer it to factor. (IS THAT REASONABLE TO DO THAT?)
<\p>

```{r}
df_cleaned$MORTLINE = as.factor(df_cleaned$MORTLINE)
```


Then I delete some meaningless columns (or predictors), where the columns only have 1 values. It is meaningless for machine learning, whatever it is supervised or unsupervised.

```{r}
for (i in 1:37) {
    
    if (length(unique(df_cleaned[[i]])) == 1) {
        df_cleaned[, i] = NA
    }
}

df_cleaned = df_cleaned %>%
    select(where(~ !all(is.na(.x))))
```

I will delete the columns `CONTROL`, since it has 15798 unique values, as a factor, I think `CONTROL` plays a role of ID, which will influence the model.

```{r}
df_cleaned = df_cleaned %>%
    select(-CONTROL)
```

## Split data to training set and test set

```{r}
set.seed(123)

set_full = df_cleaned %>%
    mutate(id = 1:16834)

set_training = set_full %>%
    slice_sample(prop = 0.8)

set_test = set_full %>%
    anti_join(set_training) %>%
    select(-id)

set_training = set_training %>%
    select(-id)
```

# Explore Data

Since our sake is to predict the interest rate of mortgage, therefore we will not do unsupervised learning, like K-Means.



## Explore the relationship between 

```{r}
require(patchwork)

plots = list()

for (i in 1:29) {
    var_name <- names(set_training)[i]
    x_var <- set_training[[i]]
  
    if (is.factor(x_var)) {
        
        p = ggplot(set_training, aes(x = factor(.data[[var_name]]), y = INTRATE)) +
        geom_boxplot() +
        theme_minimal() +
        labs(x = var_name, y = "Interest Rate")
        
    } else {
        
        p = ggplot(set_training, aes(x = .data[[var_name]], y = INTRATE)) +
        geom_point(alpha = 0.6) +
        theme_minimal() +
        labs(x = var_name, y = "Interest Rate")
        
    }
  
    plots[[i]] = p
}

print(plots)
```

From the plot, I can see that some predictors have significant association with response variable `INTRATE`, and some do not.

 * `JAMMORT` does not have significant association with `INTRATE`.
 * `JHELOCBAL` has significant association with `INTRATE`.
 * `JHELOCLIM` has significant association with `INTRATE`.
 * `JINTRATE` does not have significant association with `INTRATE`.
 * `JMISCPMT` has significant association with `INTRATE`.
 * `JMORTCLASS` does not have significant association with `INTRATE`.
 * `JMORTLINE` does not have significant association with `INTRATE`.
 * `JMORTTYPE` does not have significant association with `INTRATE`.
 * `JPMTAMT` does not have significant association with `INTRATE`.
 * `JPMTONLY` has significant association with `INTRATE`.
 * `JREFICSH` does not have significant association with `INTRATE`. But, the class `0` has more outliers.
 * `JTAXPMT` has significant association with `INTRATE`.
 * `AMMORT` has potential negative relationship between `INTRATE`.
 * `REFI` has significant association with `INTRATE`.
 * `MORTCLASS` has significant association with `INTRATE`.
 * `MORTTYPE` has significant association with `INTRATE`.
 * `TAXPMT` has significant association with `INTRATE`.
 * `PMTONLY` has potential negative relationship between `INTRATE`.
 * `MORTLINE` has significant association with `INTRATE`. 
 * `REFICSHAMT` has potential negative relationship between `INTRATE`.
 * `MORTADDTN` has potential negative relationship between `INTRATE`.
 * `HELOCLIM` has potential negative relationship between `INTRATE`.
 * `REFICSH` has significant association with `INTRATE`.
 * `HELOCBAL` has significant association with `INTRATE`.
 * `HELOCADD` has significant association with `INTRATE`.
 * `PMTFREQ` has significant association with `INTRATE`.
 * `LOANTYPE` has significant association with `INTRATE`.
 * `MISCPMT` does not have significant association with `INTRATE`.
 * `PMTAMT` has potential negative relationship between `INTRATE`

Then we will regress the model.

# Regress Model (LACK STEPWISE AND LEAPS AND BOUND select modelings.)

## Linear Model

Firstly I will do a MLR, multi-linear regress. Then I will use lasso, ridge and elastic net to shrink it.

```{r}
model_mlr_full = lm(INTRATE ~ ., data = set_training)

summary(model_mlr_full)
```

Some `NA` values appeared. It means the data, the design matrix have some multi-linear relationship between predictors. Then use Shrinkage Method is suitable.

### Stepwise

```{r}
library(MASS)
select <- dplyr::select

set.seed(123)

full_lm <- lm(INTRATE ~ ., data = set_training)
stepwise_model <- stepAIC(full_lm, direction = "both", trace = FALSE)
stepwise_preds <- predict(stepwise_model, newdata = set_test)
stepwise_rmse <- sqrt(mean((stepwise_preds - set_test$INTRATE)^2))
```

### LASSO for MLR

```{r}
require(glmnet)

set.seed(123)

X = model.matrix(INTRATE ~ ., data = set_training)[, -1]
Y = set_training$INTRATE

model_mlr_lasso = glmnet(X, Y, alpha = 1)

plot(model_mlr_lasso, xvar = "lambda", label = TRUE)
```

Use Cross-Validation to find the best $\lambda$.

```{r}
set.seed(123)

cv_model_mlr_lasso = cv.glmnet(X, Y, alpha = 1,
                               nfolds = 10,
                               type.measure = "mse",
                               thresh = 1e-10,
                               maxit = 1e6,
                               lambda.min.ratio = 1e-5,
                               nlambda = 1000,
                               standardize = FALSE)
```

### RIDGE for MLR

```{r}
set.seed(123)

model_mlr_ridge = glmnet(X, Y, alpha = 0)

plot(model_mlr_ridge, xvar = "lambda", label = TRUE)
```

Use Cross-Validation to find the best $\lambda$.

```{r}
set.seed(123)

cv_model_mlr_ridge = cv.glmnet(X, Y, alpha = 0,
                               nfolds = 10,
                               type.measure = "mse",
                               thresh = 1e-10,
                               maxit = 1e6,
                               lambda.min.ratio = 1e-5,
                               nlambda = 1000,
                               standardize = FALSE)
```


### Elastic Net for MLR

```{r}
require(tidymodels)

if (file.exists("objects/tuned_elastic_net.rds")) {
    
    model_elastic_net = readRDS("objects/model_elastic_net.rds")
    recipe_elastic_net = readRDS("objects/recipe_elastic_net.rds")
    cv_model_mlr_elastic_net = readRDS("objects/cv_model_mlr_elastic_net.rds")
    workflow_elastic_net = readRDS("objects/workflow_elastic_net.rds")
    grid_elastic_net = readRDS("objects/grid_elastic_net.rds")
    tuned_elastic_net = readRDS("objects/tuned_elastic_net.rds")
    
} else {

    set.seed(123)

    model_elastic_net = linear_reg(penalty = tune(), mixture = tune()) %>%
        set_engine("glmnet", standardize = FALSE)

    recipe_elastic_net = recipe(INTRATE ~ ., data = set_training) %>%
        step_dummy(all_nominal_predictors())

    cv_model_mlr_elastic_net = vfold_cv(set_training, v = 10)

    workflow_elastic_net = workflow() %>%
        add_model(model_elastic_net) %>%
        add_recipe(recipe_elastic_net)

    grid_elastic_net = grid_regular(
        penalty(range = c(-4, 1)),
        mixture(range = c(0, 1)),
        levels = c(penalty = 100, mixture = 100)
    )

    tuned_elastic_net = tune_grid(
        workflow_elastic_net,
        resamples = cv_model_mlr_elastic_net,
        grid = grid_elastic_net,
        metrics = metric_set(yardstick::rmse)
    )
    
    saveRDS(model_elastic_net, "objects/model_elastic_net.rds")
    saveRDS(recipe_elastic_net, "objects/recipe_elastic_net.rds")
    saveRDS(cv_model_mlr_elastic_net, "objects/cv_model_mlr_elastic_net.rds")
    saveRDS(workflow_elastic_net, "objects/workflow_elastic_net.rds")
    saveRDS(grid_elastic_net, "objects/grid_elastic_net.rds")
    saveRDS(tuned_elastic_net, "objects/tuned_elastic_net.rds")

}

best_params_elastic_net = select_best(tuned_elastic_net, metric = "rmse")
best_params_elastic_net
```

## Evaluate Models

Since our goal is to get best result of predicting interest rate, `INTRATE`. Therefore, I will evaluate the models' performance of these 4 models, MLR, Lasso and Ridge. I will use RMSE to evaluate the models' performance.

### Evaluate Stepwise

```{r}
stepwise_rmse
```

### Evaluate Linear Regression

```{r}
rmse = function(actual, pred) {
    sqrt(mean((actual - pred) ** 2))
}

pred_mlr = predict(model_mlr_full, newdata = set_test)

rmse(actual = set_test %>% pull(INTRATE), pred = pred_mlr)
```

### Evaluate Lasso Model

```{r}
X_test = model.matrix(INTRATE ~ ., data = set_test)[, -1]

pred_lasso = predict(cv_model_mlr_lasso, newx = X_test, s = cv_model_mlr_lasso$lambda.min)

rmse(actual = set_test %>% pull(INTRATE), pred = pred_lasso)
```

### Evaluate Ridge Model

```{r}
pred_ridge = predict(cv_model_mlr_ridge, newx = X_test, s = cv_model_mlr_ridge$lambda.min)

rmse(actual = set_test %>% pull(INTRATE), pred = pred_ridge)
```

### Evaluate Elastic Net

```{r}
workflow_final_elastic_net = finalize_workflow(
    workflow_elastic_net,
    best_params_elastic_net
)

final_fit = fit(workflow_final_elastic_net, data = set_training)

pred_elastic_net = predict(final_fit, new_data = set_test)$.pred

rmse(actual = set_test %>% pull(INTRATE), pred = pred_elastic_net)
```

## Summary of Linear Model

From the output, we can see that the Stepwise Model has lowest RMSE, which means its predicting is best.

# Tree Model

## A full Tree Model

```{r}
require(tree)

model_tree = tree(INTRATE ~ ., data = set_training, mindev = 1e-4)
```

Visualize it.

```{r}
plot(model_tree)
text(model_tree, cex = 0.5)
```

The tree model has to many leaves. That's because I set the `mindev = 1e-4`, which is too small. But I will keep it since I will prune the tree. I will prune by Cross-Validation, and I will use the pruned tree as model to predict data, not the full tree.

## Pruned Tree

```{r}
set.seed(123)

cv_model_tree = cv.tree(model_tree)

size_best_pruned_tree = cv_model_tree$size[cv_model_tree$dev == min(cv_model_tree$dev)]

length(size_best_pruned_tree)
```

Since there are many best size, then I can just pick one of them as the final pruned model. Also, the reason why there are so many best sizes is because the accuracy problem.

```{r}
size_best_pruned_tree = cv_model_tree$size[which.min(cv_model_tree$dev)]

model_tree_pruned = prune.tree(model_tree, best = size_best_pruned_tree)
```

## Random Forest & Bagging

Since when `m=p`, the random forest become the bagging method. Therefore we will talk them together, since I will try `m` for all possible values.

```{r, tuned_rf, cache=TRUE}
require(randomForest)

if (file.exists("objects/tuned_rf.rds")) {
    
    tuned_rf = readRDS("objects/tuned_rf.rds")
    model_rf = readRDS("objects/model_rf.rds")
    recipe_rf = readRDS("objects/recipe_rf.rds")
    cv_model_rf = readRDS("objects/cv_model_rf.rds")
    workflow_rf = readRDS("objects/workflow_rf.rds")
    grid_rf = readRDS("objects/grid_rf.rds")
    
} else {

    set.seed(123)

    model_rf = rand_forest(mtry = tune(),trees = tune(), min_n = tune()) %>%
        set_engine("randomForest") %>%
        set_mode("regression")

    recipe_rf = recipe(INTRATE ~ ., data = set_training)

    workflow_rf = workflow() %>%
        add_model(model_rf) %>%
        add_recipe(recipe_rf)

    cv_model_rf = vfold_cv(set_training, v = 10)

    grid_rf = crossing(
        mtry = 1:29,
        min_n = 5,
        trees = seq(100, 1000, by = 100)
    )

    tuned_rf = tune_grid(
        workflow_rf,
        resamples = cv_model_rf,
        grid = grid_rf,
        metrics = metric_set(yardstick::rmse),
        control = control_grid(save_pred = TRUE)
    )
    
    saveRDS(tuned_rf, "objects/tuned_rf.rds")
    saveRDS(grid_rf, "objects/grid_rf.rds")
    saveRDS(model_rf, "objects/model_rf.rds")
    saveRDS(cv_model_rf, "objects/cv_model_rf.rds")
    saveRDS(recipe_rf, "objects/recipe_rf.rds")
    saveRDS(workflow_rf, "objects/workflow_rf.rds")
}

best_params_rf = select_best(tuned_rf, metric = "rmse")
best_params_rf
```

From the output, I can see that best random forest model has 11 predictors with 900 trees. 

## Boost Model

Then I will fit a Boost Model to predict `INTRATE`. Also, I will use Cross-Validation to choose the best parameters.

```{r}
require(reticulate)

use_condaenv("xgboost_gpu", required = TRUE)

xgb = import("xgboost")
np = import("numpy")
pd = import("pandas")
sklearn_model_selection = import("sklearn.model_selection")
pickle = import("pickle")

if (file.exists("objects/tuned_boost.pkl")) {
    
    model_boost = py_load_object("objects/model_boost.pkl", pickle = "pickle")
    grid_boost = py_load_object("objects/grid_boost.pkl", pickle = "pickle")
    tuned_boost = py_load_object("objects/tuned_boost.pkl", pickle = "pickle")
    
} else {

    set.seed(123)
    
    X_train = set_training %>% select(-INTRATE)
    Y_train = set_training %>% pull(INTRATE)
    
    X_train_np = np$array(as.matrix(X_train))
    Y_train_np = np$array(Y_train)
    
    model_boost = xgb$XGBRegressor(
        objective = "reg:squarederror", # MSE will generate same results with RMSE
        tree_method = "hist",
        device = "cuda",
        verbosity = 3L
    )
    
    grid_boost = dict(
        max_depth = as.integer(c(4, 5, 6, 7, 8)),
        learning_rate = c(0.005, 0.01, 0.015,0.02),
        n_estimators = as.integer(c(300, 500, 700, 1000)),
        subsample = c(0.5, 0.6, 0.7, 0.8),
        colsample_bytree = c(0.6, 0.7, 0.8, 1.0)
    )
    
    tuned_boost = sklearn_model_selection$GridSearchCV(
        estimator = model_boost,
        param_grid = grid_boost,
        cv = 10L,
        scoring = "neg_root_mean_squared_error",
        verbose = 2L,
        n_jobs = -1L
    )
    
    tuned_boost$fit(X_train_np, Y_train_np)
    
    
    py_save_object(model_boost, "objects/model_boost.pkl", pickle = "pickle")
    py_save_object(grid_boost, "objects/grid_boost.pkl", pickle = "pickle")
    py_save_object(tuned_boost, "objects/tuned_boost.pkl", pickle = "pickle")
}

best_params_boost = tuned_boost$best_params_
best_params_boost
```

From the output, I can see that best boosting tree parameters when I use it. 


## Evaluate Model

In this part, I will still use the `rmse` to evaluate the model's performance (The predicting ability). The metric we use is the rmse, and we will compare the 2 tree models to the elastic net model.

```{r}
pred_tree = predict(model_tree_pruned, newdata = set_test)

rmse(actual = set_test %>% pull(INTRATE), pred = pred_tree)
```

From the output, I can see the pruned tree's RMSE is 1.587198.


```{r}
workflow_final_rf = finalize_workflow(workflow_rf, best_params_rf)

if (file.exists("objects/final_fit_rf.rds")) {
    
    final_fit_rf = readRDS("objects/final_fit_rf.rds")
    
} else {

    final_fit_rf = fit(workflow_final_rf, data = set_training)
    
    saveRDS(final_fit_rf, "objects/final_fit_rf.rds")
    
}

pred_rf = predict(final_fit_rf, new_data = set_test)$.pred

rmse(actual = set_test %>% pull(INTRATE), pred = pred_rf)
```

From the output, I can see that random Forest's RMSE is 1.397984, which is significantly lower than elastic net and pruned tree.

```{r}
X_test = set_test %>% select(-INTRATE)
Y_test = set_test %>% pull(INTRATE)

X_test_np = np$array(as.matrix(X_test))
Y_test_np = np$array(Y_test)

best_model_boost = tuned_boost$best_estimator_
pred_boost_np = best_model_boost$predict(X_test_np)

rmse(actual = set_test %>% pull(INTRATE), pred = pred_boost_np)
```

Therefore, I get the boosting tree's RMSE is 1.379319, which is lower than random forest. Therefore, now boosting tree is the best predicting model.


# KNN Model

## Fit Model

In this part, I will use KNN model to predict interest rate on mortgage. Also, choosing parameter `k` is a problem, therefore I will Cross-Validation to choose the best `k`.

```{r}
require(kknn)

if (file.exists("objects/tuned_knn.rds")) {
    
    model_knn = readRDS("objects/model_knn.rds")
    recipe_knn = readRDS("objects/recipe_knn.rds")
    workflow_knn = readRDS("objects/workflow_knn.rds")
    cv_model_knn = readRDS("objects/cv_model_knn.rds")
    grid_knn = readRDS("objects/grid_knn.rds")
    tuned_knn = readRDS("objects/tuned_knn.rds")
    
} else {

    set.seed(123)

    model_knn = nearest_neighbor(
        mode = "regression",
        neighbors = tune(),
        weight_func = "rectangular"
    ) %>%
        set_engine("kknn")

    recipe_knn = recipe(INTRATE ~ ., data = set_training)# %>% step_normalize(all_predictors())

    workflow_knn = workflow() %>%
        add_model(model_knn) %>%
        add_recipe(recipe_knn)

    cv_model_knn = vfold_cv(set_training, v = 10)

    grid_knn = tibble(neighbors = 1:30)

    tuned_knn = tune_grid(
        workflow_knn,
        resamples = cv_model_knn,
        grid = grid_knn,
        metrics = metric_set(yardstick::rmse),
        control = control_grid(save_pred = TRUE)
    )
    
    saveRDS(model_knn, "objects/model_knn.rds")
    saveRDS(recipe_knn, "objects/recipe_knn.rds")
    saveRDS(workflow_knn, "objects/workflow_knn.rds")
    saveRDS(cv_model_knn, "objects/cv_model_knn.rds")
    saveRDS(grid_knn, "objects/grid_knn.rds")
    saveRDS(tuned_knn, "objects/tuned_knn.rds")
}
    
best_k = select_best(tuned_knn, metric = "rmse")

best_k
```

From the output, I can see that the best kNN number for this dataset is `20`, from Cross-Validation. Therefore, I will use it to evaluate the kNN model.

## Evaluate Model

Then I will evaluate the KNN model, based on its predicting ability in test set. The metric I use is `rmse`.


```{r}
workflow_final_knn = finalize_workflow(workflow_knn, best_k)

final_fit_knn = fit(workflow_final_knn, data = set_training)

pred_knn = predict(final_fit_knn, new_data = set_test)$.pred

rmse(actual = set_test %>% pull(INTRATE), pred = pred_knn)
```

I can see that the rmse of kNN model is 1.459248. This RMSE value is significantly smaller than elastic net. But it is larger than random forest and boosting tree. Therefore, until now, the boosting tree is still the best model.


# Support Vector Regress

I will use library `kernlab` not `e1071`, since library `kernlab` is much faster than `e1071`. Also, `kernlab` support `tidymodels` therefore it is easier to tune the model.

In this question, I will try 3 kinds of kernels, which are `linear`, `polynomial` and `Radial`.

```{r}
require(kernlab)

# kernel == linear
if (file.exists("objects/tuned_svr_linear.rds")) {
    
    model_svr_linear = readRDS("objects/model_svr_linear.rds")
    recipe_svr_linear = readRDS("objects/recipe_svr_linear.rds")
    workflow_svr_linear = readRDS("objects/workflow_svr_linear.rds")
    cv_model_svr_linear = readRDS("objects/cv_model_svr_linear.rds")
    grid_svr_linear = readRDS("objects/grid_svr_linear.rds")
    tuned_svr_linear = readRDS("objects/tuned_svr_linear.rds")
    
} else {
    
    set.seed(123)
    
    model_svr_linear = svm_linear(
        mode = "regression",
        cost = tune()
    ) %>%
        set_engine("kernlab")
    
    recipe_svr_linear = recipe(INTRATE ~ ., data = set_training)# %>% step_normalize(all_predictors())
    # I do not normalize the data since I will do it later and compared them.
    
    workflow_svr_linear = workflow() %>%
        add_model(model_svr_linear) %>%
        add_recipe(recipe_svr_linear)
    
    cv_model_svr_linear = vfold_cv(set_training, v = 10)
    
    grid_svr_linear = tibble(
        cost = c(1e-4, 1e-3, 1e-2, 1e-1, 1, 1e1, 1e2, 1e3, 1e4)
    )
    
    tuned_svr_linear = tune_grid(
        workflow_svr_linear,
        resamples = cv_model_svr_linear,
        grid = grid_svr_linear,
        metrics = metric_set(yardstick::rmse),
        control = control_grid(save_pred = TRUE)
    )
    
    saveRDS(model_svr_linear, "objects/model_svr_linear.rds")
    saveRDS(recipe_svr_linear, "objects/recipe_svr_linear.rds")
    saveRDS(workflow_svr_linear, "objects/workflow_svr_linear.rds")
    saveRDS(cv_model_svr_linear, "objects/cv_model_svr_linear.rds")
    saveRDS(grid_svr_linear, "objects/grid_svr_linear.rds")
    saveRDS(tuned_svr_linear, "objects/tuned_svr_linear.rds")
    
}

# Kernel == Polynomial
if (file.exists("objects/tuned_svr_poly.rds")) {
    
    model_svr_poly = readRDS("objects/model_svr_poly.rds")
    recipe_svr_poly = readRDS("objects/recipe_svr_poly.rds")
    workflow_svr_poly = readRDS("objects/workflow_svr_poly.rds")
    cv_model_svr_poly = readRDS("objects/cv_model_svr_poly.rds")
    grid_svr_poly = readRDS("objects/grid_svr_poly.rds")
    tuned_svr_poly = readRDS("objects/tuned_svr_poly.rds")
    
} else {
    
    set.seed(123)
    
    model_svr_poly = svm_poly(
        mode = "regression",
        cost = tune(),
        degree = tune()
    ) %>%
        set_engine("kernlab")
    
    recipe_svr_poly = recipe(INTRATE ~ ., data = set_training)# %>% step_normalize(all_predictors())
    # I do not normalize the data since I will do it later and compared them.
    
    workflow_svr_poly = workflow() %>%
        add_model(model_svr_poly) %>%
        add_recipe(recipe_svr_poly)
    
    cv_model_svr_poly = vfold_cv(set_training, v = 10)
    
    grid_svr_poly = crossing(
        cost = c(1e-4, 1e-3, 1e-2, 1e-1, 1, 1e1, 1e2, 1e3, 1e4),
        degree = c(2L, 3L, 4L, 5L)
    )
    
    tuned_svr_poly = tune_grid(
        workflow_svr_poly,
        resamples = cv_model_svr_poly,
        grid = grid_svr_poly,
        metrics = metric_set(yardstick::rmse),
        control = control_grid(save_pred = TRUE)
    )
    
    saveRDS(model_svr_poly, "objects/model_svr_poly.rds")
    saveRDS(recipe_svr_poly, "objects/recipe_svr_poly.rds")
    saveRDS(workflow_svr_poly, "objects/workflow_svr_poly.rds")
    saveRDS(cv_model_svr_poly, "objects/cv_model_svr_poly.rds")
    saveRDS(grid_svr_poly, "objects/grid_svr_poly.rds")
    saveRDS(tuned_svr_poly, "objects/tuned_svr_poly.rds")
    
}


# Kernel == Radial
if (file.exists("objects/tuned_svr_rbf.rds")) {
    
    model_svr_rbf = readRDS("objects/model_svr_rbf.rds")
    recipe_svr_rbf = readRDS("objects/recipe_svr_rbf.rds")
    workflow_svr_rbf = readRDS("objects/workflow_svr_rbf.rds")
    cv_model_svr_rbf = readRDS("objects/cv_model_svr_rbf.rds")
    grid_svr_rbf = readRDS("objects/grid_svr_rbf.rds")
    tuned_svr_rbf = readRDS("objects/tuned_svr_rbf.rds")
    
} else {
    
    set.seed(123)
    
    model_svr_rbf = svm_rbf(
        mode = "regression",
        cost = tune(),
        rbf_sigma = tune()
    ) %>%
        set_engine("kernlab")
    
    recipe_svr_rbf = recipe(INTRATE ~ ., data = set_training)# %>% step_normalize(all_predictors())
    # I do not normalize the data since I will do it later and compared them.
    
    workflow_svr_rbf = workflow() %>%
        add_model(model_svr_rbf) %>%
        add_recipe(recipe_svr_rbf)
    
    cv_model_svr_rbf = vfold_cv(set_training, v = 10)
    
    grid_svr_rbf = grid_latin_hypercube(
        cost(range = c(-4, 4)),
        rbf_sigma(range = c(-3, 1)),
        size = 40
    )
    
    tuned_svr_rbf = tune_grid(
        workflow_svr_rbf,
        resamples = cv_model_svr_rbf,
        grid = grid_svr_rbf,
        metrics = metric_set(yardstick::rmse),
        control = control_grid(save_pred = TRUE)
    )
    
    saveRDS(model_svr_rbf, "objects/model_svr_rbf.rds")
    saveRDS(recipe_svr_rbf, "objects/recipe_svr_rbf.rds")
    saveRDS(workflow_svr_rbf, "objects/workflow_svr_rbf.rds")
    saveRDS(cv_model_svr_rbf, "objects/cv_model_svr_rbf.rds")
    saveRDS(grid_svr_rbf, "objects/grid_svr_rbf.rds")
    saveRDS(tuned_svr_rbf, "objects/tuned_svr_rbf.rds")
    
}


```


## Evaluate the SVR Model

In this section, I will evaluate their predicting performance by their RMSE of test value. I will test all 3 best SVR models based on different kernels.

```{r}
best_params_svr_linear = select_best(tuned_svr_linear, metric = "rmse")
best_params_svr_poly = select_best(tuned_svr_poly, metric = "rmse")
best_params_svr_rbf = select_best(tuned_svr_rbf, metric = "rmse")

workflow_final_svr_linear = finalize_workflow(workflow_svr_linear, best_params_svr_linear)
workflow_final_svr_poly = finalize_workflow(workflow_svr_poly, best_params_svr_poly)
workflow_final_svr_rbf = finalize_workflow(workflow_svr_rbf, best_params_svr_rbf)

final_fit_svr_linear = fit(workflow_final_svr_linear, data = set_training)
final_fit_svr_poly = fit(workflow_final_svr_poly, data = set_training)
final_fit_svr_rbf = fit(workflow_final_svr_rbf, data = set_training)

pred_svr_linear = predict(final_fit_svr_linear, new_data = set_test)$.pred
pred_svr_poly = predict(final_fit_svr_poly, new_data = set_test)$.pred
pred_svr_rbf = predict(final_fit_svr_rbf, new_data = set_test)$.pred

# RMSE of kernel == linear
rmse(actual = set_test %>% pull(INTRATE), pred_svr_linear)

# RMSE of kernel == polynomial
rmse(actual = set_test %>% pull(INTRATE), pred_svr_poly)

# RMSE of kernel == radian
rmse(actual = set_test %>% pull(INTRATE), pred_svr_rbf)
```

From the output, I can see that all model is worse than the random forest and boosting tree. Therefore I will not choose the SVR models.


# PCA

```{r}
numeric_cols <- set_training[, sapply(set_training, is.numeric)]
numeric_cols <- numeric_cols[, colnames(numeric_cols) != "INTRATE"]
scaled_train <- scale(numeric_cols)
test_numeric <- set_test[, sapply(set_test, is.numeric)]
test_numeric <- test_numeric[, colnames(test_numeric) != "INTRATE"]
scaled_test <- scale(test_numeric,
                     center = attr(scaled_train, "scaled:center"),
                     scale = attr(scaled_train, "scaled:scale"))
pca_model <- prcomp(scaled_train, center = FALSE, scale. = FALSE)
expl_var <- summary(pca_model)$importance["Cumulative Proportion", ]
n_pcs <- which(expl_var >= 0.9)[1]
if (is.na(n_pcs)) {
  n_pcs <- ncol(pca_model$x)
}
n_pcs <- min(n_pcs, ncol(pca_model$x))
cat("Using", n_pcs, "principal components\n")
pca_train <- as.data.frame(pca_model$x[, 1:n_pcs])
pca_train$INTRATE <- set_training$INTRATE
pca_test <- as.data.frame(as.matrix(scaled_test) %*% pca_model$rotation[, 1:n_pcs])
pca_test$INTRATE <- set_test$INTRATE

pca_lm <- lm(INTRATE ~ ., data = pca_train)
pca_preds <- predict(pca_lm, newdata = pca_test)
pca_rmse <- sqrt(mean((pca_preds - set_test$INTRATE)^2))

pca_rmse
```

From the output, I can see that PCA's RMSE is 1.667192, which is worse than Boosting Tree.


# Summary

In this unscaled data, for predicting, the final model is boosting tree. Then I will try the scaled data to get some better results. For example, the model KNN is very sensitive to scale and variance of predictors, then I will scaled data and run it again.

Also, I will not run all tree model in scale data. The reason in the below.


# Scaling data and Run again

I will scale the dataset, which makes the numerical predictors has the same mean (0) and the same variance (1). In this part, I will not do tree models, including boosting tree and random forest, since scaling has no influence on tree model.

## Data Preprocessing

```{r}
scaler_prepare = set_training %>%
    select(-INTRATE) %>%
    select(-where(is.factor)) %>%
    summarise(across(everything(), list(mean = mean, sd = sd)))

scale_method = function(data) {
    
    data_ = data
    
    data_$AMMORT = (data_$AMMORT - scaler_prepare$AMMORT_mean) / scaler_prepare$AMMORT_sd
    data_$PMTONLY = (data_$PMTONLY - scaler_prepare$PMTONLY_mean) / scaler_prepare$PMTONLY_sd
    data_$REFICSHAMT = (data_$REFICSHAMT - scaler_prepare$REFICSHAMT_mean) / scaler_prepare$REFICSHAMT_sd
    data_$MORTADDTN = (data_$MORTADDTN - scaler_prepare$MORTADDTN_mean) / scaler_prepare$MORTADDTN_sd
    data_$HELOCLIM = (data_$HELOCLIM - scaler_prepare$HELOCLIM_mean) / scaler_prepare$HELOCLIM_sd
    data_$PMTAMT = (data_$PMTAMT - scaler_prepare$PMTAMT_mean) / scaler_prepare$PMTAMT_sd
    
    return(data_)
    
}


# Seeing effect
scale_method(set_training) %>%
    select(-INTRATE) %>%
    select(-where(is.factor)) %>%
    summarise(across(everything(), list(mean = mean, sd = sd)))

```

From the output, we can see this scaling is good.

```{r}
# Deploy scaler to training and test set
set_training_scaled = scale_method(set_training)
set_test_scaled = scale_method(set_test)

```


## Linear Regression, lasso, ridge, elastic net, with scaled data

### Stepwise Linear Model

```{r}
set.seed(123)

full_lm <- lm(INTRATE ~ ., data = set_training_scaled)
stepwise_model <- stepAIC(full_lm, direction = "both", trace = FALSE)
stepwise_preds <- predict(stepwise_model, newdata = set_test_scaled)
stepwise_rmse <- sqrt(mean((stepwise_preds - set_test_scaled$INTRATE)^2))
```

### LASSO for MLR

```{r}
set.seed(123)

X_scaled = model.matrix(INTRATE ~ ., data = set_training_scaled)[, -1]
Y_scaled = set_training_scaled$INTRATE

model_mlr_lasso_scaled = glmnet(X_scaled, Y_scaled, alpha = 1)

plot(model_mlr_lasso_scaled, xvar = "lambda", label = TRUE)
```

Use Cross-Validation to find the best $\lambda$.

```{r}
set.seed(123)

cv_model_mlr_lasso_scaled = cv.glmnet(X_scaled, Y_scaled, alpha = 1,
                               nfolds = 10,
                               type.measure = "mse",
                               thresh = 1e-10,
                               maxit = 1e6,
                               lambda.min.ratio = 1e-5,
                               nlambda = 1000)
```

### RIDGE for MLR

```{r}
set.seed(123)

model_mlr_ridge_scaled = glmnet(X_scaled, Y_scaled, alpha = 0)

plot(model_mlr_ridge_scaled, xvar = "lambda", label = TRUE)
```

Use Cross-Validation to find the best $\lambda$.

```{r}
set.seed(123)

cv_model_mlr_ridge_scaled = cv.glmnet(X_scaled, Y_scaled, alpha = 0,
                               nfolds = 10,
                               type.measure = "mse",
                               thresh = 1e-10,
                               maxit = 1e6,
                               lambda.min.ratio = 1e-5,
                               nlambda = 1000)
```

### Elastic Net for MLR

```{r}
if (file.exists("scaled_objs/tuned_elastic_net_scaled.rds")) {
    
    model_elastic_net_scaled = readRDS("scaled_objs/model_elastic_net_scaled.rds")
    recipe_elastic_net_scaled = readRDS("scaled_objs/recipe_elastic_net_scaled.rds")
    cv_model_mlr_elastic_net_scaled = readRDS("scaled_objs/cv_model_mlr_elastic_net_scaled.rds")
    workflow_elastic_net_scaled = readRDS("scaled_objs/workflow_elastic_net_scaled.rds")
    grid_elastic_net_scaled = readRDS("scaled_objs/grid_elastic_net_scaled.rds")
    tuned_elastic_net_scaled = readRDS("scaled_objs/tuned_elastic_net_scaled.rds")
    
} else {

    set.seed(123)

    model_elastic_net_scaled = linear_reg(penalty = tune(), mixture = tune()) %>%
        set_engine("glmnet")

    recipe_elastic_net_scaled = recipe(INTRATE ~ ., data = set_training_scaled) %>%
        step_dummy(all_nominal_predictors())

    cv_model_mlr_elastic_net_scaled = vfold_cv(set_training_scaled, v = 10)

    workflow_elastic_net_scaled = workflow() %>%
        add_model(model_elastic_net_scaled) %>%
        add_recipe(recipe_elastic_net_scaled)

    grid_elastic_net_scaled = grid_regular(
        penalty(range = c(-4, 1)),
        mixture(range = c(0, 1)),
        levels = c(penalty = 100, mixture = 100)
    )

    tuned_elastic_net_scaled = tune_grid(
        workflow_elastic_net_scaled,
        resamples = cv_model_mlr_elastic_net_scaled,
        grid = grid_elastic_net_scaled,
        metrics = metric_set(yardstick::rmse)
    )
    
    saveRDS(model_elastic_net_scaled, "scaled_objs/model_elastic_net_scaled.rds")
    saveRDS(recipe_elastic_net_scaled, "scaled_objs/recipe_elastic_net_scaled.rds")
    saveRDS(cv_model_mlr_elastic_net_scaled, "scaled_objs/cv_model_mlr_elastic_net_scaled.rds")
    saveRDS(workflow_elastic_net_scaled, "scaled_objs/workflow_elastic_net_scaled.rds")
    saveRDS(grid_elastic_net_scaled, "scaled_objs/grid_elastic_net_scaled.rds")
    saveRDS(tuned_elastic_net_scaled, "scaled_objs/tuned_elastic_net_scaled.rds")

}

best_params_elastic_net_scaled = select_best(tuned_elastic_net_scaled, metric = "rmse")
best_params_elastic_net_scaled
```

#### Evaluate Model

### Evaluate Stepwise Model

```{r}
stepwise_rmse
```

### Evaluate Lasso Model

```{r}
X_test_scaled = model.matrix(INTRATE ~ ., data = set_test_scaled)[, -1]

pred_lasso_scaled = predict(cv_model_mlr_lasso_scaled, newx = X_test_scaled, s = cv_model_mlr_lasso_scaled$lambda.min)

rmse(actual = set_test_scaled %>% pull(INTRATE), pred = pred_lasso_scaled)
```

### Evaluate Ridge Model

```{r}
pred_ridge_scaled = predict(cv_model_mlr_ridge_scaled, newx = X_test_scaled, s = cv_model_mlr_ridge_scaled$lambda.min)

rmse(actual = set_test_scaled %>% pull(INTRATE), pred = pred_ridge_scaled)
```

### Evaluate Elastic Net

```{r}
workflow_final_elastic_net_scaled = finalize_workflow(
    workflow_elastic_net_scaled,
    best_params_elastic_net_scaled
)

final_fit_scaled = fit(workflow_final_elastic_net_scaled, data = set_training_scaled)

pred_elastic_net_scaled = predict(final_fit_scaled, new_data = set_test_scaled)$.pred

rmse(actual = set_test_scaled %>% pull(INTRATE), pred = pred_elastic_net_scaled)
```

From the output, I can see that the elastic net model is the best. But all models are worse than Boosting Tree.


## knn Model with scaled data

```{r}

if (file.exists("scaled_objs/tuned_knn_scaled.rds")) {
    
    model_knn_scaled = readRDS("scaled_objs/model_knn_scaled.rds")
    recipe_knn_scaled = readRDS("scaled_objs/recipe_knn_scaled.rds")
    workflow_knn_scaled = readRDS("scaled_objs/workflow_knn_scaled.rds")
    cv_model_knn_scaled = readRDS("scaled_objs/cv_model_knn_scaled.rds")
    grid_knn_scaled = readRDS("scaled_objs/grid_knn_scaled.rds")
    tuned_knn_scaled = readRDS("scaled_objs/tuned_knn_scaled.rds")
    
} else {

    set.seed(123)

    model_knn_scaled = nearest_neighbor(
        mode = "regression",
        neighbors = tune(),
        weight_func = "rectangular"
    ) %>%
        set_engine("kknn")

    recipe_knn_scaled = recipe(INTRATE ~ ., data = set_training_scaled)# %>% step_normalize(all_predictors())

    workflow_knn_scaled = workflow() %>%
        add_model(model_knn_scaled) %>%
        add_recipe(recipe_knn_scaled)

    cv_model_knn_scaled = vfold_cv(set_training_scaled, v = 10)

    grid_knn_scaled = tibble(neighbors = 1:30)

    tuned_knn_scaled = tune_grid(
        workflow_knn_scaled,
        resamples = cv_model_knn_scaled,
        grid = grid_knn_scaled,
        metrics = metric_set(yardstick::rmse),
        control = control_grid(save_pred = TRUE)
    )
    
    saveRDS(model_knn_scaled, "scaled_objs/model_knn_scaled.rds")
    saveRDS(recipe_knn_scaled, "scaled_objs/recipe_knn_scaled.rds")
    saveRDS(workflow_knn_scaled, "scaled_objs/workflow_knn_scaled.rds")
    saveRDS(cv_model_knn_scaled, "scaled_objs/cv_model_knn_scaled.rds")
    saveRDS(grid_knn_scaled, "scaled_objs/grid_knn_scaled.rds")
    saveRDS(tuned_knn_scaled, "scaled_objs/tuned_knn_scaled.rds")
}
    
best_k_scaled = select_best(tuned_knn_scaled, metric = "rmse")

best_k_scaled
```


### Evaluate KNN scaled Model


```{r}
workflow_final_knn_scaled = finalize_workflow(workflow_knn_scaled, best_k_scaled)

final_fit_knn_scaled = fit(workflow_final_knn_scaled, data = set_training_scaled)

pred_knn_scaled = predict(final_fit_knn_scaled, new_data = set_test_scaled)$.pred

rmse(actual = set_test_scaled %>% pull(INTRATE), pred = pred_knn_scaled)
```

From the output, we get the same result from KNN without scaling. It is still worse than boosting tree.

## Support Vector Regress

```{r}
# kernel == linear
if (file.exists("scaled_objs/tuned_svr_linear_scaled.rds")) {
    
    model_svr_linear_scaled = readRDS("scaled_objs/model_svr_linear_scaled.rds")
    recipe_svr_linear_scaled = readRDS("scaled_objs/recipe_svr_linear_scaled.rds")
    workflow_svr_linear_scaled = readRDS("scaled_objs/workflow_svr_linear_scaled.rds")
    cv_model_svr_linear_scaled = readRDS("scaled_objs/cv_model_svr_linear_scaled.rds")
    grid_svr_linear_scaled = readRDS("scaled_objs/grid_svr_linear_scaled.rds")
    tuned_svr_linear_scaled = readRDS("scaled_objs/tuned_svr_linear_scaled.rds")
    
} else {
    
    set.seed(123)
    
    model_svr_linear_scaled = svm_linear(
        mode = "regression",
        cost = tune()
    ) %>%
        set_engine("kernlab")
    
    recipe_svr_linear_scaled = recipe(INTRATE ~ ., data = set_training_scaled)
    
    workflow_svr_linear_scaled = workflow() %>%
        add_model(model_svr_linear_scaled) %>%
        add_recipe(recipe_svr_linear_scaled)
    
    cv_model_svr_linear_scaled = vfold_cv(set_training_scaled, v = 10)
    
    grid_svr_linear_scaled = tibble(
        cost = c(1e-4, 1e-3, 1e-2, 1e-1, 1, 1e1, 1e2, 1e3, 1e4)
    )
    
    tuned_svr_linear_scaled = tune_grid(
        workflow_svr_linear_scaled,
        resamples = cv_model_svr_linear_scaled,
        grid = grid_svr_linear_scaled,
        metrics = metric_set(yardstick::rmse),
        control = control_grid(save_pred = TRUE)
    )
    
    saveRDS(model_svr_linear_scaled, "scaled_objs/model_svr_linear_scaled.rds")
    saveRDS(recipe_svr_linear_scaled, "scaled_objs/recipe_svr_linear_scaled.rds")
    saveRDS(workflow_svr_linear_scaled, "scaled_objs/workflow_svr_linear_scaled.rds")
    saveRDS(cv_model_svr_linear_scaled, "scaled_objs/cv_model_svr_linear_scaled.rds")
    saveRDS(grid_svr_linear_scaled, "scaled_objs/grid_svr_linear_scaled.rds")
    saveRDS(tuned_svr_linear_scaled, "scaled_objs/tuned_svr_linear_scaled.rds")
    
}

# Kernel == Polynomial
if (file.exists("scaled_objs/tuned_svr_poly_scaled.rds")) {
    
    model_svr_poly_scaled = readRDS("scaled_objs/model_svr_poly_scaled.rds")
    recipe_svr_poly_scaled = readRDS("scaled_objs/recipe_svr_poly_scaled.rds")
    workflow_svr_poly_scaled = readRDS("scaled_objs/workflow_svr_poly_scaled.rds")
    cv_model_svr_poly_scaled = readRDS("scaled_objs/cv_model_svr_poly_scaled.rds")
    grid_svr_poly_scaled = readRDS("scaled_objs/grid_svr_poly_scaled.rds")
    tuned_svr_poly_scaled = readRDS("scaled_objs/tuned_svr_poly_scaled.rds")
    
} else {
    
    set.seed(123)
    
    model_svr_poly_scaled = svm_poly(
        mode = "regression",
        cost = tune(),
        degree = tune()
    ) %>%
        set_engine("kernlab")
    
    recipe_svr_poly_scaled = recipe(INTRATE ~ ., data = set_training_scaled)# %>% step_normalize(all_predictors())
    # I do not normalize the data since I will do it later and compared them.
    
    workflow_svr_poly_scaled = workflow() %>%
        add_model(model_svr_poly_scaled) %>%
        add_recipe(recipe_svr_poly_scaled)
    
    cv_model_svr_poly_scaled = vfold_cv(set_training_scaled, v = 10)
    
    grid_svr_poly_scaled = crossing(
        cost = c(1e-4, 1e-3, 1e-2, 1e-1, 1, 1e1, 1e2, 1e3, 1e4),
        degree = c(2L, 3L, 4L, 5L)
    )
    
    tuned_svr_poly_scaled = tune_grid(
        workflow_svr_poly_scaled,
        resamples = cv_model_svr_poly_scaled,
        grid = grid_svr_poly_scaled,
        metrics = metric_set(yardstick::rmse),
        control = control_grid(save_pred = TRUE)
    )
    
    saveRDS(model_svr_poly_scaled, "scaled_objs/model_svr_poly_scaled.rds")
    saveRDS(recipe_svr_poly_scaled, "scaled_objs/recipe_svr_poly_scaled.rds")
    saveRDS(workflow_svr_poly_scaled, "scaled_objs/workflow_svr_poly_scaled.rds")
    saveRDS(cv_model_svr_poly_scaled, "scaled_objs/cv_model_svr_poly_scaled.rds")
    saveRDS(grid_svr_poly_scaled, "scaled_objs/grid_svr_poly_scaled.rds")
    saveRDS(tuned_svr_poly_scaled, "scaled_objs/tuned_svr_poly_scaled.rds")
    
}


# Kernel == Radial
if (file.exists("scaled_objs/tuned_svr_rbf_scaled.rds")) {
    
    model_svr_rbf_scaled = readRDS("scaled_objs/model_svr_rbf_scaled.rds")
    recipe_svr_rbf_scaled = readRDS("scaled_objs/recipe_svr_rbf_scaled.rds")
    workflow_svr_rbf_scaled = readRDS("scaled_objs/workflow_svr_rbf_scaled.rds")
    cv_model_svr_rbf_scaled = readRDS("scaled_objs/cv_model_svr_rbf_scaled.rds")
    grid_svr_rbf_scaled = readRDS("scaled_objs/grid_svr_rbf_scaled.rds")
    tuned_svr_rbf_scaled = readRDS("scaled_objs/tuned_svr_rbf_scaled.rds")
    
} else {
    
    set.seed(123)
    
    model_svr_rbf_scaled = svm_rbf(
        mode = "regression",
        cost = tune(),
        rbf_sigma = tune()
    ) %>%
        set_engine("kernlab")
    
    recipe_svr_rbf_scaled = recipe(INTRATE ~ ., data = set_training_scaled)# %>% step_normalize(all_predictors())
    # I do not normalize the data since I will do it later and compared them.
    
    workflow_svr_rbf_scaled = workflow() %>%
        add_model(model_svr_rbf_scaled) %>%
        add_recipe(recipe_svr_rbf_scaled)
    
    cv_model_svr_rbf_scaled = vfold_cv(set_training_scaled, v = 10)
    
    grid_svr_rbf_scaled = grid_latin_hypercube(
        cost(range = c(-4, 4)),
        rbf_sigma(range = c(-3, 1)),
        size = 40
    )
    
    tuned_svr_rbf_scaled = tune_grid(
        workflow_svr_rbf_scaled,
        resamples = cv_model_svr_rbf_scaled,
        grid = grid_svr_rbf_scaled,
        metrics = metric_set(yardstick::rmse),
        control = control_grid(save_pred = TRUE)
    )
    
    saveRDS(model_svr_rbf_scaled, "scaled_objs/model_svr_rbf_scaled.rds")
    saveRDS(recipe_svr_rbf_scaled, "scaled_objs/recipe_svr_rbf_scaled.rds")
    saveRDS(workflow_svr_rbf_scaled, "scaled_objs/workflow_svr_rbf_scaled.rds")
    saveRDS(cv_model_svr_rbf_scaled, "scaled_objs/cv_model_svr_rbf_scaled.rds")
    saveRDS(grid_svr_rbf_scaled, "scaled_objs/grid_svr_rbf_scaled.rds")
    saveRDS(tuned_svr_rbf_scaled, "scaled_objs/tuned_svr_rbf_scaled.rds")
    
}


```

## Evaluate SVR models

```{r}
best_params_svr_linear_scaled = select_best(tuned_svr_linear_scaled, metric = "rmse")
best_params_svr_poly_scaled = select_best(tuned_svr_poly_scaled, metric = "rmse")
best_params_svr_rbf_scaled = select_best(tuned_svr_rbf_scaled, metric = "rmse")

workflow_final_svr_linear_scaled = finalize_workflow(workflow_svr_linear_scaled, best_params_svr_linear_scaled)
workflow_final_svr_poly_scaled = finalize_workflow(workflow_svr_poly_scaled, best_params_svr_poly_scaled)
workflow_final_svr_rbf_scaled = finalize_workflow(workflow_svr_rbf_scaled, best_params_svr_rbf_scaled)

final_fit_svr_linear_scaled = fit(workflow_final_svr_linear_scaled, data = set_training_scaled)
final_fit_svr_poly_scaled = fit(workflow_final_svr_poly_scaled, data = set_training_scaled)
final_fit_svr_rbf_scaled = fit(workflow_final_svr_rbf_scaled, data = set_training_scaled)

pred_svr_linear_scaled = predict(final_fit_svr_linear_scaled, new_data = set_test_scaled)$.pred
pred_svr_poly_scaled = predict(final_fit_svr_poly_scaled, new_data = set_test_scaled)$.pred
pred_svr_rbf_scaled = predict(final_fit_svr_rbf_scaled, new_data = set_test_scaled)$.pred

# RMSE of kernel == linear
rmse(actual = set_test_scaled %>% pull(INTRATE), pred_svr_linear_scaled)

# RMSE of kernel == polynomial
rmse(actual = set_test_scaled %>% pull(INTRATE), pred_svr_poly_scaled)

# RMSE of kernel == radian
rmse(actual = set_test_scaled %>% pull(INTRATE), pred_svr_rbf_scaled)
```

All scaled SVR model is worse than boosting tree.


# Summary

From all outputs, we can see that the Boosting Tree model is the best. Also, the Boosting tree model has high `mtry`, which means the model is very flexible. From the MLR model, we can see that the model has low $R^2$, which means the model is under-fit. Therefore, a model with higher flexibility will work better for this dataset, and the model Boosting Tree is best.

In boosting tree model, we use GPU to train model (Therefore we must use reticulate to use python, since R's library do not support CUDA). But we have not try the deep learning, we think we may get better results if we try deep learning.


